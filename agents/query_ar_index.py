import os, pymongo
from openai import OpenAI
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from typing import Optional
from agents.core_utils import infer_relevant_items
from agents.core_utils import get_tenk_item_descriptions
from agents.config import numCandidates, limit

def query_ar_index(query_text:str, ticker:str, filingdate: Optional[str] = None) -> str:
  """
    Retrieves an answer to a user query using annual report (10-K) filing data via a hybrid RAG (Retrieval-Augmented Generation) approach.

    This function performs the following steps:
    1. Embeds the input query using OpenAI embeddings.
    2. Infers the most relevant 10-K item codes for the query (e.g., ITEM 1A, ITEM 7A).
    3. Retrieves relevant text chunks from a vector index stored in MongoDB.
    4. Optionally filters by a specific filing date. If not provided, defaults to the latest available filing for the given ticker.
    5. Synthesizes a final answer using GPT-4o based on the retrieved excerpts.

    Parameters:
        query_text (str): The natural language question to be answered.
        ticker (str): The stock ticker symbol (e.g., "MU" for Micron).
        filingdate (Optional[str]): The annual report filing date to use (format: 'YYYY-MM-DD'). If not provided, the most recent 10-K filing is used.

    Returns:
        str: A concise, professional answer generated by the LLM based on the retrieved 10-K content.
  """
  uri = os.getenv("MONGO_URI")
  print(uri)

  # connect to your Atlas cluster
  client_mongo = pymongo.MongoClient(uri)

  # Create the query embeddings 
  client_openai = OpenAI()
  response = client_openai.embeddings.create(
      input=query_text,
      model="text-embedding-ada-002"
  )
  embedding = response.data[0].embedding

  # Find relevant item(s) from the tenk that should be used 
  item_map = get_tenk_item_descriptions()
  relevant_items = infer_relevant_items(query_text, item_map)

  # select the database and collection
  db = client_mongo["filingdb"]
  collection = db["all_filing_chunks"]
  
  if not filingdate:
    # Find the latest filing date 
    latest_filing = collection.find_one(
        {"ticker": "MU", "form": "10-K"},
        sort=[("filingdate", -1)]
    )
    if not latest_filing:
      return f"No filings found for {ticker}"
    filingdate = latest_filing["filingdate"]

  # define pipeline
  pipeline = [
    {
      '$vectorSearch': {
        'index': 'vector_index', 
        'path': 'embedding', 
        'queryVector': embedding, 
        'numCandidates': numCandidates, 
        'limit': limit, 
        'filter': {
          'item_code': {'$in': relevant_items}, 
          'filingdate': filingdate,
          'ticker': 'MU'
        }
      }
    }, {
      '$project': {
        '_id': 0, 
        'ticker': 1, 
        'filingdate': 1,
        'item_code': 1,
        'chunk': 1,
        'score': {
          '$meta': 'vectorSearchScore'
        }
      }
    }
  ]

  # run pipeline
  result_cursor = client_mongo["filingdb"]["all_filing_chunks"].aggregate(pipeline)

  retrieved_docs = list(result_cursor)
  retrieved_chunks = [doc["chunk"] for doc in retrieved_docs if "chunk" in doc]

  # Optional: Truncate or filter very long content if needed
  context = "\n\n".join(retrieved_chunks)  

  # Prepare the prompt
  final_prompt = (
    f"You are a financial analyst assistant. Based on the 10-K excerpts below, answer the question:\n\n"
    f"Question: {query_text}\n\n"
    f"Excerpts:\n{context}\n\n"
    f"Answer in a clear, concise, and professional tone suitable for an RM (Relationship Manager)."
)
  # Call the LLM to generate the final answer
  llm = ChatOpenAI(model="gpt-4o", temperature=0.2)
  final_answer = llm.invoke(final_prompt)
  
  return (final_answer.content)

def main(): 
  query_text = "what FX risks does Micron face"
  answer = query_ar_index(query_text, 'MU', filingdate=None)
  print("\nðŸ§  Final Answer:\n")
  print(answer)

if __name__ == "__main__":
    main()